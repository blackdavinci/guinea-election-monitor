#!/usr/bin/env python3
"""
Script de test pour valider une source.

Usage:
    python scripts/test_source.py "Guin√©e News"
    python scripts/test_source.py "Guin√©e News" --verbose
"""

import argparse
import sys
from pathlib import Path

# Ajouter le chemin du projet au PYTHONPATH
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

import yaml
from config.settings import CONFIG_DIR
from src.scraper.generic_scraper import GenericScraper
from src.main import load_keywords_config


def load_source_config(source_name: str) -> dict:
    """Charge la configuration d'une source."""
    sources_file = CONFIG_DIR / "sources.yaml"
    with open(sources_file, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    for source in config.get("sources", []):
        if source["name"].lower() == source_name.lower():
            return source

    return None


def test_source(source_name: str, verbose: bool = False, max_articles: int = 5):
    """Teste le scraping d'une source."""
    print("=" * 60)
    print(f"Test de la source: {source_name}")
    print("=" * 60)

    # Charger la configuration
    source_config = load_source_config(source_name)
    if not source_config:
        print(f"\n‚ùå Source '{source_name}' non trouv√©e!")
        print("\nSources disponibles:")

        sources_file = CONFIG_DIR / "sources.yaml"
        with open(sources_file, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)

        for source in config.get("sources", []):
            print(f"  - {source['name']}")
        return 1

    print(f"\nüì∞ URL de base: {source_config['base_url']}")
    print(f"üìÇ Cat√©gories: {len(source_config.get('category_urls', []))}")

    # Charger les mots-cl√©s
    keywords_dict = load_keywords_config()

    # Configurer le scraper
    selectors = source_config.get("selectors", {})
    scraper = GenericScraper(
        source_name=source_config["name"],
        base_url=source_config["base_url"],
        selectors=selectors,
        encoding=source_config.get("encoding", "utf-8"),
    )

    total_articles = 0
    all_articles = []

    # Tester chaque cat√©gorie
    for category_url in source_config.get("category_urls", []):
        print(f"\nüîç Test de: {category_url}")

        try:
            # R√©cup√©rer la liste des articles
            articles = scraper.scrape_category(
                category_url,
                keywords_dict=keywords_dict,
                max_articles=max_articles,
            )

            print(f"   ‚úÖ {len(articles)} articles trouv√©s")
            total_articles += len(articles)
            all_articles.extend(articles)

        except Exception as e:
            print(f"   ‚ùå Erreur: {e}")
            if verbose:
                import traceback
                traceback.print_exc()

    scraper.close()

    # Afficher les r√©sultats
    print("\n" + "=" * 60)
    print("R√âSULTATS")
    print("=" * 60)

    if not all_articles:
        print("\n‚ö†Ô∏è  Aucun article trouv√©!")
        print("\nConseils de d√©bogage:")
        print("1. V√©rifiez que l'URL est accessible")
        print("2. V√©rifiez les s√©lecteurs CSS")
        print("3. Le site a peut-√™tre chang√© de structure")
        return 1

    print(f"\nüìä Total articles trouv√©s: {len(all_articles)}")

    # Afficher les articles
    print("\nüì∞ Articles extraits:\n")

    for i, article in enumerate(all_articles[:10], 1):
        title = article.get("title", "Sans titre")
        url = article.get("url", "N/A")
        relevance = article.get("relevance_score", 0)
        keywords = article.get("keywords_matched", [])
        date = article.get("published_date")

        print(f"{i}. {title[:70]}...")
        print(f"   üîó {url[:80]}...")
        print(f"   üìä Pertinence: {relevance:.0%}")
        if keywords:
            print(f"   üè∑Ô∏è  Mots-cl√©s: {', '.join(keywords[:5])}")
        if date:
            print(f"   üìÖ Date: {date}")
        print()

    # Statistiques de pertinence
    relevance_scores = [a.get("relevance_score", 0) for a in all_articles]
    avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0
    high_relevance = sum(1 for s in relevance_scores if s >= 0.5)

    print("=" * 60)
    print("STATISTIQUES")
    print("=" * 60)
    print(f"üìä Pertinence moyenne: {avg_relevance:.0%}")
    print(f"‚≠ê Articles haute pertinence (‚â•50%): {high_relevance}")

    if verbose and all_articles:
        print("\n" + "=" * 60)
        print("CONTENU DU PREMIER ARTICLE")
        print("=" * 60)
        content = all_articles[0].get("content", "")[:500]
        print(f"\n{content}...")

    return 0


def main():
    parser = argparse.ArgumentParser(
        description="Tester le scraping d'une source",
    )
    parser.add_argument(
        "source_name",
        help="Nom de la source √† tester",
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Mode verbose avec plus de d√©tails",
    )
    parser.add_argument(
        "--max-articles", "-n",
        type=int,
        default=5,
        help="Nombre maximum d'articles √† r√©cup√©rer (d√©faut: 5)",
    )

    args = parser.parse_args()

    return test_source(
        args.source_name,
        verbose=args.verbose,
        max_articles=args.max_articles,
    )


if __name__ == "__main__":
    sys.exit(main())
